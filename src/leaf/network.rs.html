<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="rustdoc">
    <meta name="description" content="Source to the Rust file `src/network.rs`.">
    <meta name="keywords" content="rust, rustlang, rust-lang">

    <title>network.rs.html -- source</title>

    <link rel="stylesheet" type="text/css" href="../../rustdoc.css">
    <link rel="stylesheet" type="text/css" href="../../main.css">

    
    
</head>
<body class="rustdoc">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    

    <nav class="sidebar">
        
        
    </nav>

    <nav class="sub">
        <form class="search-form js-only">
            <div class="search-container">
                <input class="search-input" name="search"
                       autocomplete="off"
                       placeholder="Click or press ‘S’ to search, ‘?’ for more options…"
                       type="search">
            </div>
        </form>
    </nav>

    <section id='main' class="content source"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
<span id="327">327</span>
<span id="328">328</span>
<span id="329">329</span>
<span id="330">330</span>
<span id="331">331</span>
<span id="332">332</span>
<span id="333">333</span>
<span id="334">334</span>
<span id="335">335</span>
<span id="336">336</span>
<span id="337">337</span>
<span id="338">338</span>
<span id="339">339</span>
<span id="340">340</span>
<span id="341">341</span>
<span id="342">342</span>
<span id="343">343</span>
<span id="344">344</span>
<span id="345">345</span>
<span id="346">346</span>
<span id="347">347</span>
<span id="348">348</span>
<span id="349">349</span>
<span id="350">350</span>
<span id="351">351</span>
<span id="352">352</span>
<span id="353">353</span>
<span id="354">354</span>
<span id="355">355</span>
<span id="356">356</span>
<span id="357">357</span>
<span id="358">358</span>
<span id="359">359</span>
<span id="360">360</span>
<span id="361">361</span>
<span id="362">362</span>
<span id="363">363</span>
<span id="364">364</span>
<span id="365">365</span>
<span id="366">366</span>
<span id="367">367</span>
<span id="368">368</span>
<span id="369">369</span>
<span id="370">370</span>
<span id="371">371</span>
<span id="372">372</span>
<span id="373">373</span>
<span id="374">374</span>
<span id="375">375</span>
<span id="376">376</span>
<span id="377">377</span>
<span id="378">378</span>
<span id="379">379</span>
<span id="380">380</span>
<span id="381">381</span>
<span id="382">382</span>
<span id="383">383</span>
<span id="384">384</span>
<span id="385">385</span>
<span id="386">386</span>
<span id="387">387</span>
<span id="388">388</span>
<span id="389">389</span>
<span id="390">390</span>
<span id="391">391</span>
<span id="392">392</span>
<span id="393">393</span>
<span id="394">394</span>
<span id="395">395</span>
<span id="396">396</span>
<span id="397">397</span>
<span id="398">398</span>
<span id="399">399</span>
<span id="400">400</span>
<span id="401">401</span>
<span id="402">402</span>
<span id="403">403</span>
<span id="404">404</span>
<span id="405">405</span>
<span id="406">406</span>
<span id="407">407</span>
<span id="408">408</span>
<span id="409">409</span>
<span id="410">410</span>
<span id="411">411</span>
<span id="412">412</span>
<span id="413">413</span>
<span id="414">414</span>
<span id="415">415</span>
<span id="416">416</span>
<span id="417">417</span>
<span id="418">418</span>
<span id="419">419</span>
<span id="420">420</span>
<span id="421">421</span>
<span id="422">422</span>
<span id="423">423</span>
<span id="424">424</span>
<span id="425">425</span>
<span id="426">426</span>
<span id="427">427</span>
<span id="428">428</span>
<span id="429">429</span>
<span id="430">430</span>
<span id="431">431</span>
<span id="432">432</span>
<span id="433">433</span>
<span id="434">434</span>
<span id="435">435</span>
<span id="436">436</span>
<span id="437">437</span>
<span id="438">438</span>
<span id="439">439</span>
<span id="440">440</span>
<span id="441">441</span>
<span id="442">442</span>
<span id="443">443</span>
<span id="444">444</span>
<span id="445">445</span>
<span id="446">446</span>
<span id="447">447</span>
<span id="448">448</span>
<span id="449">449</span>
<span id="450">450</span>
<span id="451">451</span>
<span id="452">452</span>
<span id="453">453</span>
<span id="454">454</span>
<span id="455">455</span>
<span id="456">456</span>
<span id="457">457</span>
<span id="458">458</span>
<span id="459">459</span>
<span id="460">460</span>
<span id="461">461</span>
<span id="462">462</span>
<span id="463">463</span>
<span id="464">464</span>
<span id="465">465</span>
<span id="466">466</span>
<span id="467">467</span>
<span id="468">468</span>
<span id="469">469</span>
<span id="470">470</span>
<span id="471">471</span>
<span id="472">472</span>
<span id="473">473</span>
<span id="474">474</span>
<span id="475">475</span>
<span id="476">476</span>
<span id="477">477</span>
<span id="478">478</span>
<span id="479">479</span>
<span id="480">480</span>
<span id="481">481</span>
<span id="482">482</span>
<span id="483">483</span>
<span id="484">484</span>
<span id="485">485</span>
<span id="486">486</span>
<span id="487">487</span>
<span id="488">488</span>
<span id="489">489</span>
<span id="490">490</span>
<span id="491">491</span>
<span id="492">492</span>
<span id="493">493</span>
<span id="494">494</span>
<span id="495">495</span>
<span id="496">496</span>
<span id="497">497</span>
<span id="498">498</span>
<span id="499">499</span>
<span id="500">500</span>
<span id="501">501</span>
<span id="502">502</span>
<span id="503">503</span>
<span id="504">504</span>
<span id="505">505</span>
<span id="506">506</span>
<span id="507">507</span>
<span id="508">508</span>
<span id="509">509</span>
<span id="510">510</span>
<span id="511">511</span>
<span id="512">512</span>
<span id="513">513</span>
<span id="514">514</span>
<span id="515">515</span>
<span id="516">516</span>
<span id="517">517</span>
<span id="518">518</span>
<span id="519">519</span>
<span id="520">520</span>
<span id="521">521</span>
<span id="522">522</span>
<span id="523">523</span>
<span id="524">524</span>
<span id="525">525</span>
<span id="526">526</span>
<span id="527">527</span>
<span id="528">528</span>
<span id="529">529</span>
<span id="530">530</span>
<span id="531">531</span>
<span id="532">532</span>
<span id="533">533</span>
<span id="534">534</span>
<span id="535">535</span>
<span id="536">536</span>
<span id="537">537</span>
<span id="538">538</span>
<span id="539">539</span>
<span id="540">540</span>
<span id="541">541</span>
<span id="542">542</span>
<span id="543">543</span>
<span id="544">544</span>
<span id="545">545</span>
<span id="546">546</span>
<span id="547">547</span>
<span id="548">548</span>
<span id="549">549</span>
<span id="550">550</span>
<span id="551">551</span>
<span id="552">552</span>
<span id="553">553</span>
<span id="554">554</span>
<span id="555">555</span>
<span id="556">556</span>
<span id="557">557</span>
<span id="558">558</span>
<span id="559">559</span>
<span id="560">560</span>
<span id="561">561</span>
<span id="562">562</span>
<span id="563">563</span>
<span id="564">564</span>
<span id="565">565</span>
<span id="566">566</span>
<span id="567">567</span>
<span id="568">568</span>
<span id="569">569</span>
<span id="570">570</span>
<span id="571">571</span>
<span id="572">572</span>
<span id="573">573</span>
<span id="574">574</span>
<span id="575">575</span>
<span id="576">576</span>
<span id="577">577</span>
<span id="578">578</span>
<span id="579">579</span>
<span id="580">580</span>
<span id="581">581</span>
<span id="582">582</span>
<span id="583">583</span>
<span id="584">584</span>
<span id="585">585</span>
<span id="586">586</span>
<span id="587">587</span>
<span id="588">588</span>
<span id="589">589</span>
<span id="590">590</span>
<span id="591">591</span>
<span id="592">592</span>
<span id="593">593</span>
<span id="594">594</span>
<span id="595">595</span>
<span id="596">596</span>
<span id="597">597</span>
<span id="598">598</span>
<span id="599">599</span>
<span id="600">600</span>
<span id="601">601</span>
<span id="602">602</span>
<span id="603">603</span>
<span id="604">604</span>
<span id="605">605</span>
<span id="606">606</span>
<span id="607">607</span>
<span id="608">608</span>
<span id="609">609</span>
<span id="610">610</span>
<span id="611">611</span>
<span id="612">612</span>
<span id="613">613</span>
<span id="614">614</span>
<span id="615">615</span>
<span id="616">616</span>
<span id="617">617</span>
<span id="618">618</span>
<span id="619">619</span>
<span id="620">620</span>
<span id="621">621</span>
<span id="622">622</span>
<span id="623">623</span>
<span id="624">624</span>
<span id="625">625</span>
<span id="626">626</span>
<span id="627">627</span>
<span id="628">628</span>
<span id="629">629</span>
<span id="630">630</span>
<span id="631">631</span>
<span id="632">632</span>
<span id="633">633</span>
<span id="634">634</span>
<span id="635">635</span>
<span id="636">636</span>
<span id="637">637</span>
<span id="638">638</span>
<span id="639">639</span>
<span id="640">640</span>
<span id="641">641</span>
<span id="642">642</span>
<span id="643">643</span>
<span id="644">644</span>
<span id="645">645</span>
<span id="646">646</span>
<span id="647">647</span>
<span id="648">648</span>
<span id="649">649</span>
<span id="650">650</span>
<span id="651">651</span>
<span id="652">652</span>
<span id="653">653</span>
<span id="654">654</span>
<span id="655">655</span>
<span id="656">656</span>
<span id="657">657</span>
<span id="658">658</span>
<span id="659">659</span>
<span id="660">660</span>
<span id="661">661</span>
<span id="662">662</span>
<span id="663">663</span>
<span id="664">664</span>
<span id="665">665</span>
<span id="666">666</span>
<span id="667">667</span>
<span id="668">668</span>
<span id="669">669</span>
<span id="670">670</span>
<span id="671">671</span>
<span id="672">672</span>
<span id="673">673</span>
<span id="674">674</span>
</pre><pre class='rust '>
<span class='doccomment'>//! Provides the container of a Deep Learning Network</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! Holds all the information about its Layers, how they are connected</span>
<span class='doccomment'>//! and how the [forward][1] and [backward][2] steps should be</span>
<span class='doccomment'>//! handeled and optimized (e.g. skipping layers).</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! [1]: ./struct.Network.html#method.forward</span>
<span class='doccomment'>//! [2]: ./struct.Network.html#method.backward</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! If you are looking to train/test a network, [Solver][3] is usually a better</span>
<span class='doccomment'>//! entry point.</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ## Development</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! Currently only new networks can be created with [from_config][4].</span>
<span class='doccomment'>//! In the future there should also be a way to load networks with saved</span>
<span class='doccomment'>//! weights from a file.</span>
<span class='doccomment'>//! [Issue #14][5].</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! [3]: ../solver/index.html</span>
<span class='doccomment'>//! [4]: #method.from_config</span>
<span class='doccomment'>//! [5]: https://github.com/autumnai/leaf/issues/14</span>
<span class='doccomment'>//! [6]: https://github.com/autumnai/leaf/issues/16</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! ## Glossary</span>
<span class='doccomment'>//! ### Input Layers / Blobs</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! A input layer is the first layer of a network.&lt;/br&gt;</span>
<span class='doccomment'>//! During a forward step the data is put into the input layer,</span>
<span class='doccomment'>//! passed through all the intermediate (hidden) layers and generates a</span>
<span class='doccomment'>//! result in the output layer.</span>
<span class='doccomment'>//!</span>
<span class='doccomment'>//! The blobs in a input layer contain externally preprocessed data that has</span>
<span class='doccomment'>//! been brought into a form suitable for consumption by a neural network.</span>
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>rc</span>::<span class='ident'>Rc</span>;
<span class='kw'>use</span> <span class='ident'>co</span>::<span class='ident'>IBackend</span>;
<span class='kw'>use</span> <span class='ident'>co</span>::<span class='ident'>tensor</span>::<span class='op'>*</span>;
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>collections</span>::{<span class='ident'>HashMap</span>, <span class='ident'>HashSet</span>};
<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>sync</span>::{<span class='ident'>Arc</span>, <span class='ident'>RwLock</span>};
<span class='kw'>use</span> <span class='ident'>layer</span>::{<span class='ident'>ILayer</span>, <span class='ident'>Layer</span>};
<span class='kw'>use</span> <span class='ident'>layer</span>::<span class='ident'>LayerConfig</span>;
<span class='kw'>use</span> <span class='ident'>util</span>::{<span class='ident'>ArcLock</span>, <span class='ident'>LayerOps</span>, <span class='ident'>SolverOps</span>};

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>)]</span>
<span class='doccomment'>/// Defines a [Network][1] that contains the [Layers][2] and [Blobs][3] that store</span>
<span class='doccomment'>/// the intermediate results between the layers which are generated by [forward][4]/[backward][5].</span>
<span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Artificial_neural_network</span>
<span class='doccomment'>/// [2]: ../layer/struct.Layer.html</span>
<span class='doccomment'>/// [3]: ../../phloem/blob/struct.Blob.html</span>
<span class='doccomment'>/// [4]: ./struct.Network.html#method.forward</span>
<span class='doccomment'>/// [5]: ./struct.Network.html#method.backward</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// It is also responsible for setting up the connections between the layers.</span>
<span class='doccomment'>/// A Network is usually used together with a [Solver][6] to optimize the networks&#39; weights.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// [6]: ../solver/struct.Solver.html</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>Network</span><span class='op'>&lt;</span><span class='ident'>B</span>: <span class='ident'>IBackend</span> <span class='op'>+</span> <span class='ident'>LayerOps</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> {
    <span class='doccomment'>/// Identifies the Network</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The name is mainly used for logging purposes.</span>
    <span class='kw'>pub</span> <span class='ident'>name</span>: <span class='ident'>String</span>,
    <span class='ident'>layers</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Layer</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;&gt;</span>,

    <span class='ident'>blobs_data</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>, <span class='comment'>// the blobs storing intermediate results between the layer.</span>
    <span class='ident'>blobs_gradient</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>, <span class='comment'>// the blobs storing intermediate results between the layer.</span>
    <span class='ident'>blob_names</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,

    <span class='ident'>input_blobs_data</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>input_blobs_gradient</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>input_blob_names</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
    <span class='ident'>output_blobs_data</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>output_blobs_gradient</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,

    <span class='ident'>registry</span>: <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, (<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>)<span class='op'>&gt;</span>,

    <span class='ident'>weight_owners</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>weight_display_names</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
    <span class='ident'>weight_layer_indices</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span>(<span class='ident'>usize</span>, <span class='ident'>usize</span>)<span class='op'>&gt;</span>,
    <span class='ident'>weight_names_index</span>: <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// Defines the [parameters/weights][1] of the network.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Parameters are currently in the process of being renamed to weights throughout the codebase.</span>
    <span class='doccomment'>/// [Issue #17](https://github.com/autumnai/leaf/issues/17)</span>
    <span class='ident'>weights</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>learnable_weights_data</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>learnable_weights_gradient</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span>,
    <span class='ident'>learnable_weight_ids</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;</span>,

    <span class='ident'>weights_lr</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>,
    <span class='ident'>weights_weight_decay</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>,
}

<span class='kw'>impl</span><span class='op'>&lt;</span><span class='ident'>B</span>: <span class='ident'>IBackend</span> <span class='op'>+</span> <span class='ident'>LayerOps</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>Network</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>Network</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span> {
        <span class='ident'>Network</span> {
            <span class='ident'>name</span>: <span class='string'>&quot;&quot;</span>.<span class='ident'>to_owned</span>(),
            <span class='ident'>layers</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>blobs_data</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>blobs_gradient</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>blob_names</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>input_blobs_data</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>input_blobs_gradient</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>input_blob_names</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>output_blobs_data</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>output_blobs_gradient</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>registry</span>: <span class='ident'>HashMap</span>::<span class='ident'>new</span>(),

            <span class='ident'>weight_owners</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weight_display_names</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weight_layer_indices</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weight_names_index</span>: <span class='ident'>HashMap</span>::<span class='op'>&lt;</span><span class='ident'>String</span>, <span class='ident'>usize</span><span class='op'>&gt;</span>::<span class='ident'>new</span>(),

            <span class='ident'>weights</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>learnable_weights_data</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>learnable_weights_gradient</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>learnable_weight_ids</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],

            <span class='ident'>weights_lr</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
            <span class='ident'>weights_weight_decay</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
        }
    }
}

<span class='kw'>impl</span><span class='op'>&lt;</span><span class='ident'>B</span>: <span class='ident'>IBackend</span> <span class='op'>+</span> <span class='ident'>LayerOps</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span> <span class='op'>+</span> <span class='lifetime'>&#39;static</span><span class='op'>&gt;</span> <span class='ident'>Network</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Creates a Network from a [NetworkConfig][1].</span>
    <span class='doccomment'>/// [1]: ./struct.NetworkConfig.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// ## Examples</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// ```</span>
    <span class='doccomment'>/// # extern crate collenchyma;</span>
    <span class='doccomment'>/// # extern crate leaf;</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// # use leaf::network::*;</span>
    <span class='doccomment'>/// # use collenchyma::prelude::*;</span>
    <span class='doccomment'>/// # use std::rc::Rc;</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// # #[cfg(feature=&quot;cuda&quot;)]</span>
    <span class='doccomment'>/// # fn main() {</span>
    <span class='doccomment'>/// // create backend</span>
    <span class='doccomment'>/// let backend = Rc::new(Backend::&lt;Cuda&gt;::default().unwrap());</span>
    <span class='doccomment'>/// // create network</span>
    <span class='doccomment'>/// let cfg = NetworkConfig::default();</span>
    <span class='doccomment'>/// Network::from_config(backend, &amp;cfg);</span>
    <span class='doccomment'>/// # }</span>
    <span class='doccomment'>/// # #[cfg(not(feature=&quot;cuda&quot;))]</span>
    <span class='doccomment'>/// # fn main() {}</span>
    <span class='doccomment'>/// ```</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>from_config</span>(<span class='ident'>backend</span>: <span class='ident'>Rc</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span>, <span class='ident'>param</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>) <span class='op'>-&gt;</span> <span class='ident'>Network</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>network</span> <span class='op'>=</span> <span class='ident'>Network</span>::<span class='ident'>default</span>();
        <span class='ident'>network</span>.<span class='ident'>init</span>(<span class='ident'>backend</span>, <span class='ident'>param</span>);
        <span class='ident'>network</span>
    }

    <span class='doccomment'>/// Initializes a network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Sets up the whole structure of the network. It reads the supplied [NetworkConfig][1],</span>
    <span class='doccomment'>/// appends the top and bottom blobs to each layer and determines if the backpropagation has</span>
    <span class='doccomment'>/// to be executed for each blob and layer.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [1]: ./struct.NetworkConfig.html</span>
    <span class='kw'>fn</span> <span class='ident'>init</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>backend</span>: <span class='ident'>Rc</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span>, <span class='ident'>in_config</span>: <span class='kw-2'>&amp;</span><span class='ident'>NetworkConfig</span>) {
        <span class='kw'>let</span> <span class='ident'>config</span> <span class='op'>=</span> <span class='ident'>in_config</span>.<span class='ident'>clone</span>();
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>registry</span> <span class='op'>=</span> <span class='ident'>HashMap</span>::<span class='op'>&lt;</span><span class='ident'>String</span>, (<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>)<span class='op'>&gt;</span>::<span class='ident'>new</span>();
        <span class='kw'>let</span> <span class='ident'>weight_registry</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span>::<span class='op'>&lt;</span><span class='ident'>String</span>, (<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>, <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>)<span class='op'>&gt;</span>::<span class='ident'>new</span>();

        <span class='kw'>for</span> (<span class='ident'>input_name</span>, <span class='ident'>input_shape</span>) <span class='kw'>in</span> <span class='ident'>config</span>.<span class='ident'>inputs</span>.<span class='ident'>iter</span>().<span class='ident'>zip</span>(<span class='ident'>config</span>.<span class='ident'>input_shapes</span>.<span class='ident'>iter</span>()) {
            <span class='self'>self</span>.<span class='ident'>init_input_blob</span>(<span class='ident'>backend</span>.<span class='ident'>clone</span>(), <span class='kw-2'>&amp;</span><span class='ident'>input_name</span>, <span class='ident'>input_shape</span>, <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>registry</span>);
        }

        <span class='kw'>for</span> <span class='ident'>layer_config</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='ident'>config</span>.<span class='ident'>layers</span> {
            <span class='self'>self</span>.<span class='ident'>init_layer</span>(<span class='ident'>backend</span>.<span class='ident'>clone</span>(), <span class='kw-2'>&amp;</span><span class='ident'>layer_config</span>, <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>registry</span>, <span class='ident'>weight_registry</span>);
        }

        <span class='comment'>// Go through the net backwards to determine which blobs contribute to the</span>
        <span class='comment'>// loss.  We can skip backward computation for blobs that don&#39;t contribute</span>
        <span class='comment'>// to the loss.</span>
        <span class='comment'>// Also checks if all bottom blobs don&#39;t need backward computation (possible</span>
        <span class='comment'>// because the skip_propagate_down config) and so we can skip backward</span>
        <span class='comment'>// computation for the entire layer</span>
        <span class='kw'>let</span> <span class='ident'>blobs_under_loss</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span>::<span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>::<span class='ident'>new</span>();
        <span class='kw'>let</span> <span class='ident'>blobs_skip_backp</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashSet</span>::<span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>::<span class='ident'>new</span>();
        <span class='kw'>for</span> <span class='ident'>layer</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>iter_mut</span>().<span class='ident'>rev</span>() {
            <span class='ident'>layer</span>.<span class='ident'>init_backprop</span>( <span class='ident'>blobs_under_loss</span>, <span class='ident'>blobs_skip_backp</span>);
        }

        <span class='kw'>if</span> <span class='ident'>config</span>.<span class='ident'>force_backward</span> {
            <span class='kw'>for</span> <span class='ident'>layer</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>layers</span> {
                <span class='ident'>layer</span>.<span class='ident'>init_force_backward</span>();
            }
        }

        <span class='comment'>// In the end, all remaining blobs are considered output blobs.</span>
        <span class='kw'>for</span> (<span class='ident'>blob_name</span>, <span class='ident'>blob</span>) <span class='kw'>in</span> <span class='ident'>registry</span>.<span class='ident'>iter</span>() {
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;This network produces output {}&quot;</span>, <span class='ident'>blob_name</span>);
            <span class='self'>self</span>.<span class='ident'>output_blobs_data</span>.<span class='ident'>push</span>(<span class='ident'>blob</span>.<span class='number'>0</span>.<span class='ident'>clone</span>());
            <span class='self'>self</span>.<span class='ident'>output_blobs_gradient</span>.<span class='ident'>push</span>(<span class='ident'>blob</span>.<span class='number'>1</span>.<span class='ident'>clone</span>());
        }

        <span class='self'>self</span>.<span class='ident'>share_weights</span>();
        <span class='self'>self</span>.<span class='ident'>registry</span> <span class='op'>=</span> <span class='ident'>registry</span>;

        <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Network initialization done.&quot;</span>);
    }

    <span class='doccomment'>/// Initializes a single layer of the network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Appends [top][1] and [bottom blobs][2] to the [Layer][3]. Apart from explicitly named</span>
    <span class='doccomment'>/// top blobs it will also append anonymous top blobs that are required by the specific</span>
    <span class='doccomment'>/// [Layer implemenations][4]. It also sets up the [loss weights],</span>
    <span class='doccomment'>/// and backpropagation flags.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [1]: ../layer/index.html</span>
    <span class='doccomment'>/// [2]: ../layer/index.html</span>
    <span class='doccomment'>/// [3]: ../layer/struct.Layer.html</span>
    <span class='doccomment'>/// [4]: ../layers/index.html</span>
    <span class='kw'>fn</span> <span class='ident'>init_layer</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>,
                  <span class='ident'>backend</span>: <span class='ident'>Rc</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span>,
                  <span class='ident'>layer_config</span>: <span class='kw-2'>&amp;</span><span class='ident'>LayerConfig</span>,
                  <span class='ident'>registry</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, (<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>)<span class='op'>&gt;</span>,
                  <span class='ident'>weight_registry</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, (<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>, <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;</span>)<span class='op'>&gt;</span>) {

        <span class='comment'>// Setup layer.</span>
        <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Err</span>(<span class='ident'>e</span>) <span class='op'>=</span> <span class='ident'>layer_config</span>.<span class='ident'>validate</span>() {
            <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;{}&quot;</span>, <span class='ident'>e</span>);
        }

        <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Creating Layer {}&quot;</span>, <span class='ident'>layer_config</span>.<span class='ident'>name</span>.<span class='ident'>clone</span>());
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>layer</span> <span class='op'>=</span> <span class='ident'>Layer</span>::<span class='ident'>from_config</span>(<span class='ident'>backend</span>, <span class='kw-2'>&amp;</span><span class='ident'>layer_config</span>);

        <span class='comment'>// Figure out this layer&#39;s input and output</span>
        <span class='ident'>layer</span>.<span class='ident'>connect</span>(<span class='ident'>registry</span>, <span class='ident'>weight_registry</span>);
        <span class='kw'>for</span> <span class='ident'>weight_data</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='ident'>layer</span>.<span class='ident'>weights_data</span> {
            <span class='self'>self</span>.<span class='ident'>learnable_weights_data</span>.<span class='ident'>push</span>(<span class='ident'>weight_data</span>.<span class='ident'>clone</span>());
        }
        <span class='kw'>for</span> <span class='ident'>weight_gradient</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='ident'>layer</span>.<span class='ident'>weights_gradient</span> {
            <span class='self'>self</span>.<span class='ident'>learnable_weights_gradient</span>.<span class='ident'>push</span>(<span class='ident'>weight_gradient</span>.<span class='ident'>clone</span>());
        }

        <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>push</span>(<span class='ident'>layer</span>);
    }

    <span class='doccomment'>/// Share weights among multiple layers.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Shared weights are usually used for [Siamese networks][1]</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [1]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.4792</span>
    <span class='kw'>fn</span> <span class='ident'>share_weights</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='comment'>// Caffe / not sure if ported correctly</span>
        <span class='comment'>// for (int i = 0; i &lt; params_.size(); ++i) {</span>
        <span class='comment'>//     if (param_owners_[i] &lt; 0) { continue; }</span>
        <span class='comment'>//     params_[i]-&gt;ShareData(*params_[param_owners_[i]]);</span>
        <span class='comment'>//     params_[i]-&gt;ShareDiff(*params_[param_owners_[i]]);</span>
        <span class='comment'>// }</span>
        <span class='kw'>for</span> (<span class='ident'>i</span>, _) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>weights</span>.<span class='ident'>clone</span>().<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='kw'>if</span> <span class='kw'>let</span> <span class='prelude-val'>Some</span>(<span class='ident'>j</span>) <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weight_owners</span>[<span class='ident'>i</span>] {
                <span class='macro'>assert</span><span class='macro'>!</span>(<span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>i</span>].<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>desc</span>().<span class='ident'>size</span>() <span class='op'>==</span>
                        <span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>j</span>].<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>desc</span>().<span class='ident'>size</span>());
                <span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>i</span>] <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>weights</span>[<span class='ident'>j</span>].<span class='ident'>clone</span>(); <span class='comment'>// sharing whole blob?</span>
            }
        }
    }

    <span class='doccomment'>/// Initialize input blobs for the Network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Appends a input blob to the network, so the bottom-most [Layer][1] can</span>
    <span class='doccomment'>/// [connect][2] to them.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Used during initialization of the Network.</span>
    <span class='doccomment'>/// [1]: ../layer/struct.Layer.html</span>
    <span class='doccomment'>/// [2]: ../layer/struct.Layer.html#method.connect</span>
    <span class='attribute'>#[<span class='ident'>cfg_attr</span>(<span class='ident'>lint</span>, <span class='ident'>allow</span>(<span class='ident'>ptr_arg</span>))]</span>
    <span class='kw'>fn</span> <span class='ident'>init_input_blob</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>,
                  <span class='ident'>backend</span>: <span class='ident'>Rc</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span>,
                  <span class='ident'>blob_name</span>: <span class='kw-2'>&amp;</span><span class='ident'>str</span>,
                  <span class='ident'>input_shape</span>: <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;</span>,
                  <span class='ident'>registry</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>HashMap</span><span class='op'>&lt;</span><span class='ident'>String</span>, (<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>, <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>)<span class='op'>&gt;</span> ) {

        <span class='kw'>if</span> <span class='ident'>registry</span>.<span class='ident'>contains_key</span>(<span class='ident'>blob_name</span>) {
            <span class='comment'>// If we are not doing in-place computation but have duplicated blobs, raise an</span>
            <span class='comment'>// error.</span>
            <span class='macro'>error</span><span class='macro'>!</span>(<span class='string'>&quot;Top blob {} produced by multiple sources.&quot;</span>, <span class='ident'>blob_name</span>);
            <span class='kw'>return</span>
        } <span class='kw'>else</span> {
            <span class='macro'>info</span><span class='macro'>!</span>(<span class='string'>&quot;Input {} -&gt; {}&quot;</span>, <span class='self'>self</span>.<span class='ident'>input_blobs_data</span>.<span class='ident'>len</span>(), <span class='ident'>blob_name</span>);

            <span class='kw'>let</span> <span class='ident'>ibackend</span>: <span class='ident'>Rc</span><span class='op'>&lt;</span><span class='ident'>IBackend</span><span class='op'>&lt;</span><span class='ident'>F</span><span class='op'>=</span><span class='ident'>B</span>::<span class='ident'>F</span><span class='op'>&gt;&gt;</span> <span class='op'>=</span> <span class='ident'>backend</span>;
            <span class='kw'>let</span> <span class='ident'>blob_data</span>: <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> <span class='op'>=</span> <span class='ident'>Arc</span>::<span class='ident'>new</span>(<span class='ident'>RwLock</span>::<span class='ident'>new</span>(<span class='ident'>SharedTensor</span>::<span class='ident'>new</span>(<span class='ident'>ibackend</span>.<span class='ident'>device</span>(), <span class='ident'>input_shape</span>).<span class='ident'>unwrap</span>()));
            <span class='kw'>let</span> <span class='ident'>blob_gradient</span>: <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> <span class='op'>=</span> <span class='ident'>Arc</span>::<span class='ident'>new</span>(<span class='ident'>RwLock</span>::<span class='ident'>new</span>(<span class='ident'>SharedTensor</span>::<span class='ident'>new</span>(<span class='ident'>ibackend</span>.<span class='ident'>device</span>(), <span class='ident'>input_shape</span>).<span class='ident'>unwrap</span>()));
            <span class='kw'>let</span> <span class='ident'>blob_id</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>blobs_data</span>.<span class='ident'>len</span>();
            <span class='self'>self</span>.<span class='ident'>blobs_data</span>.<span class='ident'>push</span>(<span class='ident'>blob_data</span>.<span class='ident'>clone</span>());
            <span class='self'>self</span>.<span class='ident'>blob_names</span>.<span class='ident'>push</span>(<span class='ident'>blob_name</span>.<span class='ident'>to_owned</span>());

            <span class='self'>self</span>.<span class='ident'>input_blobs_data</span>.<span class='ident'>push</span>(<span class='ident'>blob_data</span>.<span class='ident'>clone</span>());
            <span class='self'>self</span>.<span class='ident'>input_blob_names</span>.<span class='ident'>push</span>(<span class='ident'>blob_name</span>.<span class='ident'>to_owned</span>());
            <span class='ident'>registry</span>.<span class='ident'>insert</span>(<span class='ident'>blob_name</span>.<span class='ident'>to_owned</span>(), (<span class='ident'>blob_data</span>, <span class='ident'>blob_gradient</span>));
        }
    }

    <span class='doccomment'>/// Computes [forward][1] and [backward][2] step for the network and returns [the total loss.][3]</span>
    <span class='doccomment'>/// [1]: #method.forward</span>
    <span class='doccomment'>/// [2]: #method.backward</span>
    <span class='doccomment'>/// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Used by the [Solver][4] to conveniently compute one [forward- and one backward-propagation</span>
    <span class='doccomment'>/// step][5] together, which is all the network has to do while training it.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [4]: ../solver/struct.Solver.html</span>
    <span class='doccomment'>/// [5]: https://en.wikipedia.org/wiki/Backpropagation#Phase_1:_Propagation</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward_backward</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>bottom</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>]) <span class='op'>-&gt;</span> <span class='ident'>f32</span> {
        <span class='kw'>let</span> <span class='ident'>loss</span> <span class='op'>=</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='number'>0f32</span>;

        <span class='self'>self</span>.<span class='ident'>forward</span>(<span class='ident'>bottom</span>, <span class='ident'>loss</span>);
        <span class='self'>self</span>.<span class='ident'>backward</span>();

        <span class='op'>*</span><span class='ident'>loss</span>
    }

    <span class='doccomment'>/// Copies supplied [input Blobs][1] into the network, computes [forward step][2] for the</span>
    <span class='doccomment'>/// network and returns [the output blobs.][3].</span>
    <span class='doccomment'>/// [1]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>/// [2]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='doccomment'>/// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Does not actually copy data, only references to the input blobs.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// This is the go-to if you just want to feed data to your network and get the corresponding</span>
    <span class='doccomment'>/// output.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>input</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>], <span class='ident'>loss</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>f32</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span> {
        <span class='kw'>for</span> (<span class='ident'>i</span>, <span class='ident'>inp</span>) <span class='kw'>in</span> <span class='ident'>input</span>.<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
            <span class='self'>self</span>.<span class='ident'>input_blobs_data</span>[<span class='ident'>i</span>] <span class='op'>=</span> <span class='ident'>inp</span>.<span class='ident'>clone</span>();
            <span class='kw'>for</span> <span class='ident'>layer</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>layers</span> {
                <span class='kw'>for</span> (<span class='ident'>blob_index</span>, <span class='ident'>blob_name</span>) <span class='kw'>in</span> <span class='ident'>layer</span>.<span class='ident'>input_blob_names</span>().<span class='ident'>to_owned</span>().<span class='ident'>iter</span>().<span class='ident'>enumerate</span>() {
                    <span class='kw'>if</span> <span class='ident'>blob_name</span> <span class='op'>==</span> <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>input_blob_names</span>[<span class='ident'>i</span>] {
                        <span class='kw'>let</span> <span class='ident'>reshaped_shape</span> <span class='op'>=</span> <span class='ident'>layer</span>.<span class='ident'>input_blobs_data</span>[<span class='ident'>blob_index</span>].<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>desc</span>().<span class='ident'>clone</span>();
                        <span class='ident'>layer</span>.<span class='ident'>input_blobs_data</span>[<span class='ident'>blob_index</span>] <span class='op'>=</span> <span class='ident'>inp</span>.<span class='ident'>clone</span>();
                        <span class='comment'>// reshape input tensor to the reshaped shape</span>
                        <span class='kw'>let</span> <span class='ident'>old_shape</span> <span class='op'>=</span> <span class='ident'>layer</span>.<span class='ident'>input_blobs_data</span>[<span class='ident'>blob_index</span>].<span class='ident'>read</span>().<span class='ident'>unwrap</span>().<span class='ident'>desc</span>().<span class='ident'>clone</span>();
                        <span class='kw'>if</span> <span class='ident'>old_shape</span>.<span class='ident'>size</span>() <span class='op'>!=</span> <span class='ident'>reshaped_shape</span>.<span class='ident'>size</span>() {
                            <span class='macro'>panic</span><span class='macro'>!</span>(<span class='string'>&quot;The provided input does not have the expected shape&quot;</span>);
                        }
                        <span class='ident'>layer</span>.<span class='ident'>input_blobs_data</span>[<span class='ident'>blob_index</span>].<span class='ident'>write</span>().<span class='ident'>unwrap</span>().<span class='ident'>reshape</span>(<span class='kw-2'>&amp;</span><span class='ident'>reshaped_shape</span>).<span class='ident'>unwrap</span>();
                    }
                }
            }
        }

        <span class='self'>self</span>.<span class='ident'>forward_prefilled</span>(<span class='prelude-val'>Some</span>(<span class='ident'>loss</span>))
    }

    <span class='doccomment'>/// Computes [forward step][1] for a network whose [input blob][2] references have been set</span>
    <span class='doccomment'>/// and returns [the output blobs.][3]</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='doccomment'>/// [2]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>/// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Can be used if you need more control over how to put data into the network (debugging),</span>
    <span class='doccomment'>/// otherwise [forward][4] is the prefered method to forward through the whole network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [4]: #method.forward</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward_prefilled</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>loss</span>: <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>f32</span><span class='op'>&gt;</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span> {
        <span class='kw'>let</span> <span class='ident'>end</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>();
        <span class='kw'>match</span> <span class='ident'>loss</span> {
            <span class='prelude-val'>Some</span>(<span class='ident'>loss_result</span>) <span class='op'>=&gt;</span> {
                <span class='comment'>// not sure if loss_result will really be changed</span>
                <span class='op'>*</span><span class='ident'>loss_result</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>forward_from_to</span>(<span class='number'>0</span>, <span class='ident'>end</span>);
            }
            <span class='prelude-val'>None</span> <span class='op'>=&gt;</span> {
                <span class='self'>self</span>.<span class='ident'>forward_from_to</span>(<span class='number'>0</span>, <span class='ident'>end</span>);
            }
        }

        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>output_blobs_data</span>
    }

    <span class='doccomment'>/// Compute [forward step][1] for a part of (or the whole) network and returns the [total loss][2].</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class='doccomment'>/// [2]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Computes the forward step from the layer with index `start` to the layer with index `end`</span>
    <span class='doccomment'>/// and return the total [scalar loss][2] over all loss layers.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// If you want to compute a foward step for the whole network</span>
    <span class='doccomment'>/// you should use [forward_prefilled][3].</span>
    <span class='doccomment'>/// Computing a forward on a part of the network is usually only done for debugging purposes.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [3]: #method.forward_prefilled</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>forward_from_to</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>start</span>: <span class='ident'>usize</span>, <span class='ident'>end</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='ident'>f32</span> {
        <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>end</span> <span class='op'>&lt;=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>());

        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>loss</span> <span class='op'>=</span> <span class='number'>0f32</span>;

        <span class='kw'>for</span> <span class='ident'>i</span> <span class='kw'>in</span> <span class='ident'>start</span>..<span class='ident'>end</span> {
            <span class='ident'>loss</span> <span class='op'>+=</span> <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>forward</span>();
            <span class='kw'>if</span> <span class='ident'>i</span> <span class='op'>==</span> (<span class='ident'>end</span> <span class='op'>-</span> <span class='number'>1</span>) {
                <span class='comment'>// synchronize after last layer</span>
                <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>synchronize</span>();
            }
        }
        <span class='macro'>debug</span><span class='macro'>!</span>(<span class='string'>&quot;LOSS {:?}&quot;</span>, <span class='ident'>loss</span>);

        <span class='ident'>loss</span>
    }

    <span class='doccomment'>/// Computes a [backpropagation][1] step for the whole network using the currently set output blobs.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Computes the backpropagation step for each layer of the Network using [backward_from_to][2].</span>
    <span class='doccomment'>/// [2]: #method.backward_from_to</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Called directly only for debugging purposes.</span>
    <span class='doccomment'>/// Backpropagating a network is only useful during training and handled by a [Solver][3]</span>
    <span class='doccomment'>/// [3]: ../solver/index.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>backward</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>let</span> <span class='ident'>start</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>();
        <span class='macro'>debug</span><span class='macro'>!</span>(<span class='string'>&quot;BACKWARD NETWORK START: {:?}&quot;</span>, <span class='kw-2'>&amp;</span><span class='ident'>start</span>);
        <span class='self'>self</span>.<span class='ident'>backward_input_from_to</span>(<span class='ident'>start</span>, <span class='number'>0</span>);
        <span class='self'>self</span>.<span class='ident'>backward_parameters_from_to</span>(<span class='ident'>start</span>, <span class='number'>0</span>);
    }

    <span class='doccomment'>/// TODO: Docs</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>backward_input</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>let</span> <span class='ident'>start</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>();
        <span class='self'>self</span>.<span class='ident'>backward_input_from_to</span>(<span class='ident'>start</span>, <span class='number'>0</span>);
    }

    <span class='doccomment'>/// TODO: Docs</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>backward_parameters</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>let</span> <span class='ident'>start</span> <span class='op'>=</span> <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>len</span>();
        <span class='self'>self</span>.<span class='ident'>backward_parameters_from_to</span>(<span class='ident'>start</span>, <span class='number'>0</span>);
    }

    <span class='doccomment'>/// Compute [backpropagation][1] step for a part of (or the whole) network.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Computes the backpropagation step from the layer with index `start` to the layer with index `end`,</span>
    <span class='doccomment'>/// skipping layers that have been flagged to be skipped (usually in [init_backprop][2]).</span>
    <span class='doccomment'>/// [2]: #method.init_backprop</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// If you want to compute a foward step for the whole network you should use [backward][3].</span>
    <span class='doccomment'>/// Computing a backward on a part of the network is usually only done for debugging purposes.</span>
    <span class='doccomment'>/// [3]: #method.backward</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>backward_input_from_to</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>start</span>: <span class='ident'>usize</span>, <span class='ident'>end</span>: <span class='ident'>usize</span>) {
        <span class='comment'>// assert!(start &lt; self.layers.len());</span>
        <span class='macro'>debug</span><span class='macro'>!</span>(<span class='string'>&quot;BACKWARD NETWORK LAYERS&quot;</span>);
        <span class='kw'>for</span> <span class='ident'>i</span> <span class='kw'>in</span> (<span class='ident'>end</span>..<span class='ident'>start</span>).<span class='ident'>rev</span>() {
            <span class='macro'>debug</span><span class='macro'>!</span>(<span class='string'>&quot;BACKWARD NETWORK LAYER {:?}&quot;</span>, <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>name</span>);
            <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>backward_input</span>();
            <span class='kw'>if</span> <span class='ident'>i</span> <span class='op'>==</span> <span class='ident'>end</span> {
                <span class='comment'>// synchronize after last layer</span>
                <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>synchronize</span>();
            }
        }
    }

    <span class='doccomment'>/// TODO: Docs</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>backward_parameters_from_to</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>start</span>: <span class='ident'>usize</span>, <span class='ident'>end</span>: <span class='ident'>usize</span>) {
        <span class='macro'>debug</span><span class='macro'>!</span>(<span class='string'>&quot;BACKWARD NETWORK LAYERS&quot;</span>);
        <span class='kw'>for</span> <span class='ident'>i</span> <span class='kw'>in</span> (<span class='ident'>end</span>..<span class='ident'>start</span>).<span class='ident'>rev</span>() {
            <span class='macro'>debug</span><span class='macro'>!</span>(<span class='string'>&quot;BACKWARD NETWORK LAYER {:?}&quot;</span>, <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>name</span>);
            <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>backward_parameters</span>();
            <span class='kw'>if</span> <span class='ident'>i</span> <span class='op'>==</span> <span class='ident'>end</span> {
                <span class='comment'>// synchronize after last layer</span>
                <span class='self'>self</span>.<span class='ident'>layers</span>[<span class='ident'>i</span>].<span class='ident'>synchronize</span>();
            }
        }
    }

    <span class='doccomment'>/// Clears the [weights][1] diffs and zero-inits them.</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The diffs for the weights accumulate over the backpropagation steps of</span>
    <span class='doccomment'>/// a [Solver][2] minibatch and are cleared between each minibatch</span>
    <span class='doccomment'>/// to start over with a clean slate.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [2]: ../solver/struct.Solver.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>clear_weight_diffs</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>) {
        <span class='kw'>for</span> <span class='ident'>weight_gradient</span> <span class='kw'>in</span> <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>learnable_weights_gradient</span>.<span class='ident'>iter</span>() {
            <span class='kw'>let</span> <span class='ident'>filler</span> <span class='op'>=</span> ::<span class='ident'>weight</span>::<span class='ident'>FillerType</span>::<span class='ident'>Constant</span> {
                <span class='ident'>value</span>: <span class='number'>0f32</span>
            };
            <span class='ident'>filler</span>.<span class='ident'>fill</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>weight_gradient</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>());
        }
    }
}

<span class='kw'>impl</span><span class='op'>&lt;</span><span class='ident'>B</span>: <span class='ident'>IBackend</span> <span class='op'>+</span> <span class='ident'>LayerOps</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> <span class='ident'>Network</span><span class='op'>&lt;</span><span class='ident'>B</span><span class='op'>&gt;</span> {
    <span class='doccomment'>/// Updates the [weights][1] with the weight update computed by the [Solver][2].</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class='doccomment'>/// [2]: ../solver/struct.Solver.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Updating the weights is the last step of computing a [Solver][2] minibatch.</span>
    <span class='doccomment'>/// The update value is computed in previous steps according to the [learning rate policy][3]</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [3]: ../solver/enum.LRPolicy.html</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>update_weights</span><span class='op'>&lt;</span><span class='ident'>SolverB</span>: <span class='ident'>IBackend</span> <span class='op'>+</span> <span class='ident'>SolverOps</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>backend</span>: <span class='kw-2'>&amp;</span><span class='ident'>SolverB</span>) {
        <span class='kw'>let</span> <span class='kw-2'>mut</span> <span class='ident'>shared_a</span> <span class='op'>=</span> ::<span class='ident'>util</span>::<span class='ident'>native_scalar</span>(<span class='op'>-</span><span class='number'>1f32</span>);
        <span class='kw'>let</span> _ <span class='op'>=</span> <span class='ident'>shared_a</span>.<span class='ident'>add_device</span>(<span class='ident'>backend</span>.<span class='ident'>device</span>());
        <span class='ident'>shared_a</span>.<span class='ident'>sync</span>(<span class='ident'>backend</span>.<span class='ident'>device</span>()).<span class='ident'>unwrap</span>();
        <span class='kw'>for</span> (<span class='ident'>weight_gradient</span>, <span class='ident'>weight_data</span>) <span class='kw'>in</span> <span class='self'>self</span>.<span class='ident'>learnable_weights_gradient</span>.<span class='ident'>iter</span>().<span class='ident'>zip</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>.<span class='ident'>learnable_weights_data</span>) {
            <span class='ident'>weight_gradient</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>().<span class='ident'>sync</span>(<span class='ident'>backend</span>.<span class='ident'>device</span>()).<span class='ident'>unwrap</span>();
            <span class='ident'>weight_data</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>().<span class='ident'>sync</span>(<span class='ident'>backend</span>.<span class='ident'>device</span>()).<span class='ident'>unwrap</span>();
            <span class='ident'>backend</span>.<span class='ident'>axpy_plain</span>(<span class='kw-2'>&amp;</span><span class='ident'>shared_a</span>, <span class='kw-2'>&amp;</span><span class='ident'>weight_gradient</span>.<span class='ident'>read</span>().<span class='ident'>unwrap</span>(), <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>weight_data</span>.<span class='ident'>write</span>().<span class='ident'>unwrap</span>()).<span class='ident'>unwrap</span>();
            <span class='comment'>// weight_blob.write().unwrap().apply_diff(backend) // TODO: solver</span>
        }
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>learnable_weight_data</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span> {
        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>learnable_weights_data</span>
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>learnable_weight_gradients</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span><span class='op'>&gt;</span> {
        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>learnable_weights_gradient</span>
    }

    <span class='doccomment'>/// get the data associated with the provided tensor name</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>get_data</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>name</span>: <span class='kw-2'>&amp;</span><span class='ident'>str</span>) <span class='op'>-&gt;</span> <span class='ident'>ArcLock</span><span class='op'>&lt;</span><span class='ident'>SharedTensor</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>registry</span>.<span class='ident'>get</span>(<span class='ident'>name</span>).<span class='ident'>unwrap</span>().<span class='number'>0</span>.<span class='ident'>clone</span>()
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>weights_weight_decay</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> {
        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>weights_weight_decay</span>
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>weights_lr</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>) <span class='op'>-&gt;</span> <span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='ident'>f32</span><span class='op'>&gt;&gt;</span> {
        <span class='kw-2'>&amp;</span><span class='self'>self</span>.<span class='ident'>weights_lr</span>
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Defines the configuration of a network.</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// TODO: [DOC] When and why would you use this?</span>
<span class='doccomment'>/// TODO: [DOC] What is the purpose of this configuration type?</span>
<span class='doccomment'>///</span>
<span class='doccomment'>/// TODO: [DOC] &lt;Now-What&gt; Examples</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>NetworkConfig</span> {
    <span class='doccomment'>/// Defines the name the network.</span>
    <span class='kw'>pub</span> <span class='ident'>name</span>: <span class='ident'>String</span>,

    <span class='doccomment'>/// Defines the names of the [input blobs][1].</span>
    <span class='doccomment'>/// [1]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The input blobs are identified by name so they can be referenced as [input blobs][2]</span>
    <span class='doccomment'>/// in a [LayerConfig][3].</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// [2]: ../layer/index.html</span>
    <span class='doccomment'>/// [3]: ../layer/struct.LayerConfig.html</span>
    <span class='kw'>pub</span> <span class='ident'>inputs</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,

    <span class='doccomment'>/// Defines the [shape][1] of the [input blobs][2].</span>
    <span class='doccomment'>/// [1]: ???</span>
    <span class='doccomment'>/// [2]: ./index.html#input-layers--blobs</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// The number of input_shapes supplied should match the number of inputs supplied.</span>
    <span class='doccomment'>/// The shape of the input blobs has to be known so that the right connections to the</span>
    <span class='doccomment'>/// upper layers can be set up.</span>
    <span class='kw'>pub</span> <span class='ident'>input_shapes</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span>,

    <span class='doccomment'>/// Defines if the network will force every layer to do [backpropagation][1].</span>
    <span class='doccomment'>/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// If set to `false`, then the execution of backpropagation is determined automatically</span>
    <span class='doccomment'>/// according to the net structure and learning rates.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: `false`</span>
    <span class='kw'>pub</span> <span class='ident'>force_backward</span>: <span class='ident'>bool</span>,

    <span class='doccomment'>/// Defines the [state][1] of the network.</span>
    <span class='doccomment'>/// [1]: ../struct.NetworkState.html</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Some layers may be included/excluded depending on this state and the states</span>
    <span class='doccomment'>/// specified in the layers&#39; include and exclude fields.</span>
    <span class='kw'>pub</span> <span class='ident'>state</span>: <span class='ident'>NetworkState</span>,

    <span class='doccomment'>/// Defines if the network will print debugging information about results</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: `false`</span>
    <span class='kw'>pub</span> <span class='ident'>debug_info</span>: <span class='ident'>bool</span>,

    <span class='doccomment'>/// Defines the layers of the network via [LayerConfig][1]s.</span>
    <span class='doccomment'>/// [1]: ../layer/struct.LayerConfig.html</span>
    <span class='kw'>pub</span> <span class='ident'>layers</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>LayerConfig</span><span class='op'>&gt;</span>,
}

<span class='kw'>impl</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>NetworkConfig</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>NetworkConfig</span> {
        <span class='ident'>NetworkConfig</span> {
            <span class='ident'>name</span>: <span class='string'>&quot;&quot;</span>.<span class='ident'>to_owned</span>(),
            <span class='ident'>inputs</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
            <span class='ident'>input_shapes</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),

            <span class='ident'>force_backward</span>: <span class='boolval'>false</span>,
            <span class='ident'>debug_info</span>: <span class='boolval'>false</span>,

            <span class='ident'>layers</span>: <span class='ident'>Vec</span>::<span class='ident'>new</span>(),
            <span class='ident'>state</span>: <span class='ident'>NetworkState</span>::<span class='ident'>default</span>(),
        }
    }
}

<span class='kw'>impl</span> <span class='ident'>NetworkConfig</span> {
    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>layer</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>layer_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>LayerConfig</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>get</span>(<span class='ident'>layer_id</span>)
    }

    <span class='doccomment'>/// Add layer at the end of the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>add_layer</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>layer</span>: <span class='ident'>LayerConfig</span>) {
        <span class='self'>self</span>.<span class='ident'>layers</span>.<span class='ident'>push</span>(<span class='ident'>layer</span>);
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>input</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>input_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>String</span><span class='op'>&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>inputs</span>.<span class='ident'>get</span>(<span class='ident'>input_id</span>)
    }

    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>input_shape</span>(<span class='kw-2'>&amp;</span><span class='self'>self</span>, <span class='ident'>input_id</span>: <span class='ident'>usize</span>) <span class='op'>-&gt;</span> <span class='prelude-ty'>Option</span><span class='op'>&lt;</span><span class='kw-2'>&amp;</span><span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>usize</span><span class='op'>&gt;&gt;</span> {
        <span class='self'>self</span>.<span class='ident'>input_shapes</span>.<span class='ident'>get</span>(<span class='ident'>input_id</span>)
    }

    <span class='doccomment'>/// Add a input to the network.</span>
    <span class='kw'>pub</span> <span class='kw'>fn</span> <span class='ident'>add_input</span>(<span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='self'>self</span>, <span class='ident'>input_name</span>: <span class='kw-2'>&amp;</span><span class='ident'>str</span>, <span class='ident'>shape</span>: <span class='kw-2'>&amp;</span>[<span class='ident'>usize</span>]) {
        <span class='self'>self</span>.<span class='ident'>inputs</span>.<span class='ident'>push</span>(<span class='ident'>input_name</span>.<span class='ident'>to_owned</span>());
        <span class='self'>self</span>.<span class='ident'>input_shapes</span>.<span class='ident'>push</span>(<span class='ident'>shape</span>.<span class='ident'>to_owned</span>());
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Defines the state of a network.</span>
<span class='kw'>pub</span> <span class='kw'>struct</span> <span class='ident'>NetworkState</span> {
    <span class='doccomment'>/// Defines the current mode of the network.</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: Test</span>
    <span class='kw'>pub</span> <span class='ident'>mode</span>: <span class='ident'>NetworkMode</span>,
    <span class='doccomment'>/// TODO: [DOC] what does this do?</span>
    <span class='doccomment'>/// TODO: [DOC] could it be of type usize?</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: 0</span>
    <span class='kw'>pub</span> <span class='ident'>level</span>: <span class='ident'>isize</span>,
    <span class='doccomment'>/// TODO: [DOC] what does this do?</span>
    <span class='doccomment'>///</span>
    <span class='doccomment'>/// Default: vec![]</span>
    <span class='kw'>pub</span> <span class='ident'>stage</span>: <span class='ident'>Vec</span><span class='op'>&lt;</span><span class='ident'>String</span><span class='op'>&gt;</span>,
}

<span class='kw'>impl</span> <span class='ident'>Default</span> <span class='kw'>for</span> <span class='ident'>NetworkState</span> {
    <span class='kw'>fn</span> <span class='ident'>default</span>() <span class='op'>-&gt;</span> <span class='ident'>NetworkState</span> {
        <span class='ident'>NetworkState</span> {
            <span class='ident'>mode</span>: <span class='ident'>NetworkMode</span>::<span class='ident'>Test</span>,
            <span class='ident'>level</span>: <span class='number'>0</span>,
            <span class='ident'>stage</span>: <span class='macro'>vec</span><span class='macro'>!</span>[],
        }
    }
}

<span class='attribute'>#[<span class='ident'>derive</span>(<span class='ident'>Debug</span>, <span class='ident'>Copy</span>, <span class='ident'>Clone</span>)]</span>
<span class='doccomment'>/// Defines the possible modes that a network can be in.</span>
<span class='kw'>pub</span> <span class='kw'>enum</span> <span class='ident'>NetworkMode</span> {
    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='ident'>Train</span>,
    <span class='attribute'>#[<span class='ident'>allow</span>(<span class='ident'>missing_docs</span>)]</span>
    <span class='ident'>Test</span>,
}
</pre>
</section>
    <section id='search' class="content hidden"></section>

    <section class="footer"></section>

    <aside id="help" class="hidden">
        <div>
            <h1 class="hidden">Help</h1>

            <div class="shortcuts">
                <h2>Keyboard Shortcuts</h2>

                <dl>
                    <dt>?</dt>
                    <dd>Show this help dialog</dd>
                    <dt>S</dt>
                    <dd>Focus the search field</dd>
                    <dt>&larrb;</dt>
                    <dd>Move up in search results</dd>
                    <dt>&rarrb;</dt>
                    <dd>Move down in search results</dd>
                    <dt>&#9166;</dt>
                    <dd>Go to active search result</dd>
                </dl>
            </div>

            <div class="infos">
                <h2>Search Tricks</h2>

                <p>
                    Prefix searches with a type followed by a colon (e.g.
                    <code>fn:</code>) to restrict the search to a given type.
                </p>

                <p>
                    Accepted types are: <code>fn</code>, <code>mod</code>,
                    <code>struct</code>, <code>enum</code>,
                    <code>trait</code>, <code>type</code>, <code>macro</code>,
                    and <code>const</code>.
                </p>

                <p>
                    Search functions by type signature (e.g.
                    <code>vec -> usize</code>)
                </p>
            </div>
        </div>
    </aside>

    

    <script>
        window.rootPath = "../../";
        window.currentCrate = "leaf";
        window.playgroundUrl = "";
    </script>
    <script src="../../jquery.js"></script>
    <script src="../../main.js"></script>
    
    <script defer src="../../search-index.js"></script>
</body>
</html>